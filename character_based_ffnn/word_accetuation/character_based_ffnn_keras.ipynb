{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import unicode_literals\n",
    "# text in Western (Windows 1252)\n",
    "\n",
    "import numpy as np\n",
    "# import StringIO\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import regularizers\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import load_model\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run ../../prepare_data.py\n",
    "\n",
    "# import sys\n",
    "# sys.path.insert(0, '../../')\n",
    "# import preprare_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "additional_data = [el[2] for el in content]\n",
    "\n",
    "possible_variants = sorted(set(additional_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vmps-sna\n",
      "Vmip1s--n\n",
      "Vmip1p--n\n",
      "Vmip2s--n\n",
      "Vmu\n",
      "Vmip2d--n\n",
      "Vmip3d--n\n",
      "Vmip2p--n\n",
      "Vmn\n",
      "Vmip1d--n\n",
      "Rgp\n",
      "Afpmsnn\n"
     ]
    }
   ],
   "source": [
    "def decode_X_features(feature_dictionary, X_other_features):\n",
    "    for word in X_other_features:\n",
    "        final_word = []\n",
    "        i = 0\n",
    "        for z in range(len(feature_dictionary)):\n",
    "            for j in range(1, len(feature_dictionary[z])):\n",
    "                if j == 1:\n",
    "                    if word[i] == 1:\n",
    "#                         print feature_dictionary[z][1]\n",
    "                        final_word.append(feature_dictionary[z][1])\n",
    "                    i += 1\n",
    "                else:\n",
    "                    for k in range(len(feature_dictionary[z][j])):\n",
    "                        if word[i] == 1:\n",
    "#                             print feature_dictionary[z][j][k]\n",
    "                            final_word.append(feature_dictionary[z][j][k])\n",
    "                        i += 1\n",
    "        print(u''.join(final_word))\n",
    "decode_X_features(feature_dictionary, X_other_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(536431, 167)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X_other_features).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = 3\n",
    "\n",
    "def create_X_features(content, feature_dictionary):\n",
    "    content = content\n",
    "    X_other_features = []\n",
    "    for el in content:\n",
    "        X_el_other_features = []\n",
    "        for feature in feature_dictionary:\n",
    "            if el[2][0] == feature[1]:\n",
    "                X_el_other_features.append(1)\n",
    "                for i in range(2, len(feature)):\n",
    "                    for j in range(len(feature[i])):\n",
    "                        if i-1 < len(el[2]) and feature[i][j] == el[2][i-1]:\n",
    "                            X_el_other_features.append(1)\n",
    "                        else:\n",
    "                            X_el_other_features.append(0)\n",
    "#                 X_other_features.append(X_el_other_features)\n",
    "                        \n",
    "            else:\n",
    "                X_el_other_features.extend([0] * feature[0])\n",
    "#         break\n",
    "#         print X_el_other_features\n",
    "        X_other_features.append(X_el_other_features)\n",
    "    return X_other_features\n",
    "        \n",
    "X_other_features = create_X_features(content, feature_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[25,\n",
       "  u'A',\n",
       "  [u'f', u'o', u's'],\n",
       "  [u'c', u'p', u's'],\n",
       "  [u'f', u'm', u'n'],\n",
       "  [u'd', u'p', u's'],\n",
       "  [u'a', u'd', u'g', u'i', u'l', u'n'],\n",
       "  [u'-', u'n', u'y'],\n",
       "  [u'-'],\n",
       "  [u'n', u'y']],\n",
       " [5, u'C', [u'c', u's'], [u'c', u's']],\n",
       " [1, u'I'],\n",
       " [25,\n",
       "  u'M',\n",
       "  [u'c', u'm', u'o', u's'],\n",
       "  [u'f', u'm', u'n'],\n",
       "  [u'd', u'p', u's'],\n",
       "  [u'a', u'd', u'g', u'i', u'l', u'n'],\n",
       "  [u'l'],\n",
       "  [u'-', u'n', u'y'],\n",
       "  [u'-'],\n",
       "  [u'-'],\n",
       "  [u'n', u'y']],\n",
       " [18,\n",
       "  u'N',\n",
       "  [u'c'],\n",
       "  [u'f', u'm', u'n'],\n",
       "  [u'd', u'p', u's'],\n",
       "  [u'a', u'd', u'g', u'i', u'l', u'n'],\n",
       "  [u'-'],\n",
       "  [u'-'],\n",
       "  [u'n', u'y']],\n",
       " [48,\n",
       "  u'P',\n",
       "  [u'd', u'g', u'i', u'p', u'q', u'r', u's', u'x', u'z'],\n",
       "  [u'-', u'1', u'2', u'3'],\n",
       "  [u'-', u'f', u'm', u'n'],\n",
       "  [u'-', u'd', u'p', u's'],\n",
       "  [u'-', u'a', u'd', u'g', u'i', u'l', u'n'],\n",
       "  [u'-', u'd', u'p', u's'],\n",
       "  [u'-', u'f', u'm', u'n'],\n",
       "  [u'-', u'n', u'y'],\n",
       "  [u'-', u'p', u's'],\n",
       "  [u'a', u'n'],\n",
       "  [u'-'],\n",
       "  [u'n', u'y']],\n",
       " [1, u'Q'],\n",
       " [5, u'R', [u'g'], [u'c', u'p', u's']],\n",
       " [9, u'S', [u'p'], [u'c', u's'], [u'a', u'd', u'g', u'i', u'l']],\n",
       " [30,\n",
       "  u'V',\n",
       "  [u'c', u'm', u'o'],\n",
       "  [u'c', u'i', u'm', u'n', u'p', u'u'],\n",
       "  [u'-', u'f', u'p', u's'],\n",
       "  [u'-', u'1', u'2', u'3'],\n",
       "  [u'd', u'p', u's'],\n",
       "  [u'-', u'f', u'm', u'n'],\n",
       "  [u'-', u'a', u'p'],\n",
       "  [u'n', u'y']]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_feature_dictionary(content):\n",
    "    additional_data = [el[2] for el in content]\n",
    "    possible_variants = sorted(set(additional_data))\n",
    "    categories = sorted(set([el[0] for el in possible_variants]))\n",
    "#     print categories\n",
    "\n",
    "    # [all_atts[i] for i in range(1,len(all_atts))]\n",
    "\n",
    "    specifics = [el for el in possible_variants if el[0] == 'A']\n",
    "#     print sorted(set([el[1] for el in specifics if el]))\n",
    "    # len(possible_variants)\n",
    "    # len(specifics)\n",
    "    # print specifics\n",
    "    feature_dictionary = []\n",
    "    for category in categories:\n",
    "        category_features = [1, category]\n",
    "        examples_per_category = [el for el in possible_variants if el[0] == category]\n",
    "        longest_element = max(examples_per_category, key=len)\n",
    "        for i in range(1, len(longest_element)):\n",
    "            possibilities_per_el = sorted(set([el[i] for el in examples_per_category if i < len(el)]))\n",
    "            category_features[0] += len(possibilities_per_el)\n",
    "            category_features.append(possibilities_per_el)\n",
    "        feature_dictionary.append(category_features)\n",
    "    #     print longest_element\n",
    "    return feature_dictionary\n",
    "\n",
    "feature_dictionary = create_feature_dictionary(content)\n",
    "feature_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING CONTENT...\n",
      "CONTENT READ SUCCESSFULY\n",
      "CREATING DICTIONARY...\n",
      "DICTIONARY CREATION SUCCESSFUL!\n",
      "GENERATING X AND y...\n"
     ]
    }
   ],
   "source": [
    "# X, y = generate_inputs()\n",
    "# save_inputs('ordered_inputs.h5', X, y)\n",
    "# X, y = load_inputs('ordered_inputs.h5')\n",
    "\n",
    "# X, y = generate_matrix_inputs()\n",
    "# save_inputs('ordered_matrix_inputs.h5', X, y)\n",
    "# X, y = load_inputs('ordered_matrix_inputs.h5')\n",
    "\n",
    "X, y = generate_full_matrix_inputs()\n",
    "X_train, y_train, X_validate, y_validate = generate_full_matrix_inputs()\n",
    "save_inputs('../internal_representations/inputs/shuffeled_full_matrix_train_inputs.h5', X_train, y_train)\n",
    "save_inputs('../internal_representations/inputs/shuffeled_full_matrix_validate_inputs.h5', X_validate, y_validate)\n",
    "# X_train, y_train = load_inputs('../internal_representations/inputs/shuffeled_full_matrix_train_inputs.h5')\n",
    "# X_validate, y_validate = load_inputs('../internal_representations/inputs/shuffeled_full_matrix_validate_inputs.h5')\n",
    "\n",
    "# X, y, X_pure = generate_full_vowel_matrix_inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X = X[:100000]\n",
    "# y = y[:100000]\n",
    "# def unison_shuffled_copies(a, b):\n",
    "#     assert len(a) == len(b)\n",
    "#     p = np.random.permutation(len(a))\n",
    "#     return a[p], b[p]\n",
    "# X, y = unison_shuffled_copies(X, y)\n",
    "# print X.shape\n",
    "\n",
    "# train_X = X.astype(np.float32)\n",
    "# train_y = y.astype(np.int32)\n",
    "\n",
    "num_examples = len(X_train) # training set size\n",
    "# nn_input_dim = max_word * len(dictionary) # input layer dimensionality\n",
    "# nn_output_dim = max_num_vowels * max_num_vowels # output layer dimensionality\n",
    "nn_output_dim = 121\n",
    "nn_hdim = 516\n",
    "\n",
    "# Gradient descent parameters (I picked these by hand)\n",
    "# epsilon = 1 # learning rate for gradient descent\n",
    "# reg_lambda = 1 # regularization strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "np.random.seed(7)\n",
    "\n",
    "\n",
    "# create model\n",
    "# model = Sequential()\n",
    "# model.add(Conv1D(43, (3), input_shape=(23, 43), padding='same', activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Conv1D(43, (3), padding='same', activation='relu'))\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(Conv1D(86, (3), padding='same', activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Conv1D(86, (3), padding='same', activation='relu'))\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(Conv1D(172, (3), padding='same', activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Conv1D(172, (3), padding='same', activation='relu'))\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# # model.add(Conv1D(43, (3), input_shape=(None, 43), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(1032, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(516, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(nn_output_dim, activation='softmax'))\n",
    "\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(43, (3), input_shape=(23, 43), padding='same', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(43, (3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(Conv1D(43, (3), input_shape=(None, 43), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "model.add(Flatten())\n",
    "# model.add(Dense(1032, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(516, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(nn_output_dim, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "lrate = 0.1\n",
    "decay = lrate/epochs\n",
    "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Compile model\n",
    "# model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 428142 samples, validate on 108290 samples\n",
      "Epoch 1/3\n",
      "428142/428142 [==============================] - 1020s - loss: 0.0022 - acc: 0.8302 - val_loss: 0.0022 - val_acc: 0.8293\n",
      "Epoch 2/3\n",
      "428142/428142 [==============================] - 1554s - loss: 0.0011 - acc: 0.9169 - val_loss: 0.0020 - val_acc: 0.8451\n",
      "Epoch 3/3\n",
      "  1010/428142 [..............................] - ETA: 1818s - loss: 0.0010 - acc: 0.9158"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-92a910bb42d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# testY = y[:536432/16]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# print testY.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_validate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_validate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# model.fit(X, y, epochs=5, validation_split=0.2, batch_size=10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    843\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1483\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1138\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.5/site-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.5/site-packages/theano/ifelse.py\u001b[0m in \u001b[0;36mthunk\u001b[0;34m()\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mthunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model.fit_generator((X, y), 536432/2, epochs=5)\n",
    "# testX = X[:536432/16]\n",
    "# print testX.shape\n",
    "# testY = y[:536432/16]\n",
    "# print testY.shape\n",
    "model.fit(X_train, y_train, validation_data=(X_validate, y_validate), epochs=3, batch_size=10)\n",
    "# model.fit(X, y, epochs=5, validation_split=0.2, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('cnn_i2_s_c43-3relu_d20_c43-3relu_mp2_f_516relu_d20_121sigmoid_mse_adam_a65_3epoch_no_overfitting.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model('../internal_representations/models/cnn_i2_s_c43-3relu_d20_c43-3relu_mp2_f_516relu_d20_121sigmoid_mse_adam_a65_10epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h5f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e361f7023d53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'h5f' is not defined"
     ]
    }
   ],
   "source": [
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107232/107287 [============================>.] - ETA: 0s\n",
      "acc: 96.52%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X[429145:], y[429145:])\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 0]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# calculate predictions\n",
    "# test = generate_input_from_word('test', max_word, dictionary)\n",
    "# print test.shape\n",
    "# print test.T.shape\n",
    "predictions = model.predict(np.array([generate_input_from_word('biotski', max_word, dictionary)]))\n",
    "# round predictions\n",
    "\n",
    "print decode_position(predictions[0], max_num_vowels)\n",
    "\n",
    "b_pred = 0\n",
    "ind = 0\n",
    "i = 0\n",
    "for el in predictions[0]:\n",
    "    if b_pred < el:\n",
    "        b_pred = el\n",
    "        ind = i\n",
    "    i += 1\n",
    "    \n",
    "print(ind)\n",
    "# rounded = [round(x[0]) for x in predictions]\n",
    "# print(rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X[429145:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_position(predictions[0], max_num_vowels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING CONTENT...\n",
      "CONTENT READ SUCCESSFULY\n",
      "CREATING DICTIONARY...\n",
      "DICTIONARY CREATION SUCCESSFUL!\n",
      "107287\n",
      "96.5186835311\n"
     ]
    }
   ],
   "source": [
    "def test_accuracy(predictions, y):\n",
    "    dictionary, max_word, max_num_vowels, content, vowels, accetuated_vowels = create_dict()\n",
    "    num_of_pred = len(predictions)\n",
    "    num_of_correct_pred = 0\n",
    "    for i in range(predictions.shape[0]):\n",
    "        if decode_position(predictions[i], max_num_vowels) == decode_position(y[i], max_num_vowels):\n",
    "            num_of_correct_pred += 1\n",
    "\n",
    "    return (num_of_correct_pred/float(num_of_pred)) * 100\n",
    "\n",
    "\n",
    "print(test_accuracy(predictions, y[429145:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107287, 121)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "print max_num_vowels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING CONTENT...\n",
      "CONTENT READ SUCCESSFULY\n",
      "CREATING DICTIONARY...\n",
      "DICTIONARY CREATION SUCCESSFUL!\n"
     ]
    }
   ],
   "source": [
    "dictionary, max_word, max_num_vowels, content, vowels, accetuated_vowels = create_dict()\n",
    "def generate_input_from_word(word, max_word, dictionary):\n",
    "    x = np.zeros((max_word, len(dictionary)))\n",
    "    j = 0\n",
    "    for c in list(word):\n",
    "        index = 0\n",
    "        for d in dictionary:\n",
    "            if c == d:\n",
    "                x[j, index] = 1\n",
    "                break\n",
    "            index += 1\n",
    "        j += 1\n",
    "    return x\n",
    "\n",
    "# model = load_model()\n",
    "# prediction = predict(model, generate_input_from_word('hidrija'))\n",
    "# print decode_position(prediction[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
