{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import unicode_literals\n",
    "# text in Western (Windows 1252)\n",
    "\n",
    "import numpy as np\n",
    "import StringIO, math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Merge\n",
    "from keras import regularizers\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import load_model\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../prepare_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING CONTENT...\n",
      "CONTENT READ SUCCESSFULY\n",
      "CREATING DICTIONARY...\n",
      "DICTIONARY CREATION SUCCESSFUL!\n",
      "Shuffled vector loaded!\n",
      "CREATED 1. PART OF SHUFFLED MATRIX\n",
      "CREATED 2. PART OF SHUFFLED MATRIX\n",
      "CREATED 3. PART OF SHUFFLED MATRIX\n",
      "CREATED 4. PART OF SHUFFLED MATRIX\n",
      "CREATED 5. PART OF SHUFFLED MATRIX\n",
      "CREATED 6. PART OF SHUFFLED MATRIX\n"
     ]
    }
   ],
   "source": [
    "shuffle_full_vowel_inputs('internal_representations/inputs/inputs_shuffeled2.h5', 'internal_representations/inputs/X_ordered_part.h5', 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h5f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-5fb4da9d9ce4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# del X, y, X_pure, test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'h5f' is not defined"
     ]
    }
   ],
   "source": [
    "# del X, y, X_pure, test\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING CONTENT...\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: u'../../data/SlovarIJS_BESEDE_utf8.lex'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f6f11f4ff6a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_num_vowels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvowels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccetuated_vowels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/luka/Developement/accetuation/prepare_data.py\u001b[0m in \u001b[0;36mcreate_dict\u001b[1;34m()\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m     \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'CREATING DICTIONARY...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/luka/Developement/accetuation/prepare_data.py\u001b[0m in \u001b[0;36mread_content\u001b[1;34m()\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'READING CONTENT...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../../data/SlovarIJS_BESEDE_utf8.lex'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'CONTENT READ SUCCESSFULY'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: u'../../data/SlovarIJS_BESEDE_utf8.lex'"
     ]
    }
   ],
   "source": [
    "dictionary, max_word, max_num_vowels, content, vowels, accetuated_vowels = create_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object generate_inputs_from_file at 0x7f3c85f2cf00>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time, math\n",
    "\n",
    "\n",
    "def generate_inputs_from_file(path, batch_size):\n",
    "    h5f = h5py.File(path, 'r')\n",
    "    finnish = False\n",
    "#     s = []\n",
    "    while 1:\n",
    "        indices_range = [0, batch_size]\n",
    "        i = 0\n",
    "#         print 'HERE NOOOOOOOOOOOO!!!'\n",
    "        while i < len(h5f['X'])/float(batch_size):\n",
    "#             print 'HERE!!!'\n",
    "#             start_time = time.time()\n",
    "#             if randomize:\n",
    "#                 indices = s[indices_range[0]:indices_range[1]]\n",
    "#                 indices = np.sort(indices)\n",
    "#                 print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "#                 start_time = time.time()\n",
    "#                 X = h5f['X'][indices.tolist()]\n",
    "#                 y = h5f['y'][indices.tolist()]\n",
    "#                 X_pure = h5f['X_pure'][indices.tolist()]\n",
    "#                 print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "                \n",
    "#             else:\n",
    "            X = h5f['X'][indices_range[0]:indices_range[1]]\n",
    "            y = h5f['y'][indices_range[0]:indices_range[1]]\n",
    "            X_pure = h5f['X_pure'][indices_range[0]:indices_range[1]]\n",
    "#             print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "            indices_range[0] = indices_range[1]\n",
    "            if indices_range[1] + batch_size < len(h5f['X']):\n",
    "                indices_range[1] = indices_range[1] + batch_size\n",
    "            else:\n",
    "                indices_range[1] = len(h5f['X'])\n",
    "            i += 1\n",
    "            yield [X, X_pure], y\n",
    "#             break\n",
    "#             h5f.close()\n",
    "#             finnish = True\n",
    "#             break\n",
    "#             yield (X, y, X_pure)\n",
    "#         if finnish:\n",
    "#             yield\n",
    "        \n",
    "    \n",
    "test = generate_inputs_from_file('internal_representations/inputs/inputs_shuffeled.h5', 10)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "[X, X_pure], y = test.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "èrnogledostma\n",
      "4\n",
      "0\n",
      "1\n",
      "najslavnejših\n",
      "3\n",
      "0\n",
      "2\n",
      "planinskem\n",
      "1\n",
      "1\n",
      "3\n",
      "obraèunavanjem\n",
      "2\n",
      "0\n",
      "4\n",
      "konservativne\n",
      "2\n",
      "0\n",
      "5\n",
      "rajnkima\n",
      "2\n",
      "0\n",
      "6\n",
      "birokratova\n",
      "1\n",
      "0\n",
      "7\n",
      "razobešenimi\n",
      "5\n",
      "0\n",
      "8\n",
      "zasluženimi\n",
      "1\n",
      "1\n",
      "9\n",
      "smotrnejše\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# print len(X)\n",
    "i = 7\n",
    "for i in range(len(X)):\n",
    "    print i\n",
    "    print decode_input(X[i], dictionary)\n",
    "    print X_pure[i]\n",
    "    print y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018553\n"
     ]
    }
   ],
   "source": [
    "h5f = h5py.File('internal_representations/inputs/X_ordered_part.h5', 'r')\n",
    "print(len(h5f['X']))\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING CONTENT...\n",
      "CONTENT READ SUCCESSFULY\n",
      "CREATING DICTIONARY...\n",
      "DICTIONARY CREATION SUCCESSFUL!\n",
      "GENERATING X AND y...\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "Saving part 1\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "Saving part 2\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "Saving part 3\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "Saving part 4\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "Saving part 5\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "Saving part 6\n"
     ]
    }
   ],
   "source": [
    "generate_full_vowel_matrix_inputs('internal_representations/inputs/X_ordered_part', 6)\n",
    "# dictionary, max_word, max_num_vowels, content, vowels, accetuated_vowels = create_dict()\n",
    "# print('HERE!')\n",
    "# num = count_vowels(content, vowels)\n",
    "# print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zvedavima\n",
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "h5f = h5py.File('internal_representations/inputs/X_ordered_part.h5','r')\n",
    "# print h5f['X'][500000]\n",
    "i = 2000002\n",
    "print decode_input(h5f['X'][i], dictionary)\n",
    "print h5f['X_pure'][i]\n",
    "print h5f['y'][i]\n",
    "h5f.close()\n",
    "\n",
    "# def decode_input(word_encoded, dictionary):\n",
    "#     word = ''\n",
    "#     for el in word_encoded:\n",
    "#         i = 0\n",
    "#         for num in el:\n",
    "#             if num == 1:\n",
    "#                 word += dictionary[i]\n",
    "#                 break\n",
    "#             i += 1\n",
    "#     return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING CONTENT...\n",
      "CONTENT READ SUCCESSFULY\n",
      "CREATING DICTIONARY...\n",
      "DICTIONARY CREATION SUCCESSFUL!\n",
      "Shuffled vector loaded!"
     ]
    }
   ],
   "source": [
    "shuffle_full_vowel_inputs('internal_representations/inputs/X_shuffled_part', 'internal_representations/inputs/X_ordered_part', 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = generate_inputs()\n",
    "# save_inputs('ordered_inputs.h5', X, y)\n",
    "# X, y = load_inputs('ordered_inputs.h5')\n",
    "\n",
    "# X, y = generate_matrix_inputs()\n",
    "# save_inputs('ordered_matrix_inputs.h5', X, y)\n",
    "# X, y = load_inputs('ordered_matrix_inputs.h5')\n",
    "\n",
    "# X, y = generate_full_matrix_inputs()\n",
    "# save_inputs('shuffeled_full_matrix_inputs.h5', X, y)\n",
    "X, y = load_inputs('internal_representations/inputs/shuffeled_full_matrix_inputs.h5')\n",
    "\n",
    "# X, y, X_pure = generate_full_vowel_matrix_inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING CONTENT...\n",
      "CONTENT READ SUCCESSFULY\n",
      "CREATING DICTIONARY...\n",
      "DICTIONARY CREATION SUCCESSFUL!"
     ]
    }
   ],
   "source": [
    "testX, testY = generate_full_vowel_matrix_inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 2018553 # training set size\n",
    "batch_size = 16\n",
    "nn_output_dim = 1\n",
    "nn_hdim = 516\n",
    "\n",
    "# Gradient descent parameters (I picked these by hand)\n",
    "# epsilon = 1 # learning rate for gradient descent\n",
    "# reg_lambda = 1 # regularization strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luka/Developement/nn-from-scratch/venv/lib/python2.7/site-packages/ipykernel/__main__.py:32: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "# np.random.seed(7)\n",
    "\n",
    "\n",
    "word_processor = Sequential()\n",
    "word_processor.add(Conv1D(43, (3), input_shape=(23, 43), padding='same', activation='relu'))\n",
    "# word_processor.add(Dropout(0.2))\n",
    "word_processor.add(Conv1D(43, (3), padding='same', activation='relu'))\n",
    "word_processor.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(Conv1D(43, (3), input_shape=(None, 43), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "word_processor.add(Flatten())\n",
    "# model.add(Dense(1032, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "# model.add(Dropout(0.2))\n",
    "word_processor.add(Dense(516, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "\n",
    "\n",
    "metadata_processor = Sequential()\n",
    "metadata_processor.add(Dense(16, input_dim=1, activation='relu'))\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([word_processor, metadata_processor], mode='concat'))  # Merge is your sensor fusion buddy\n",
    "model.add(Dense(516, input_dim=532, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(1032, input_dim=532, activation='relu'))       \n",
    "# # model.add(Activation('relu'))\n",
    "# model.add(Dense(number_of_classes))\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# create model\n",
    "# model = Sequential()\n",
    "# model.add(Conv1D(43, (3), input_shape=(23, 43), padding='same', activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Conv1D(43, (3), padding='same', activation='relu'))\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(Conv1D(86, (3), padding='same', activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Conv1D(86, (3), padding='same', activation='relu'))\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(Conv1D(172, (3), padding='same', activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Conv1D(172, (3), padding='same', activation='relu'))\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# # model.add(Conv1D(43, (3), input_shape=(None, 43), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(1032, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(516, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(nn_output_dim, activation='softmax'))\n",
    "\n",
    "\n",
    "# create model\n",
    "# model = Sequential()\n",
    "# model.add(Conv1D(43, (3), input_shape=(23, 43), padding='same', activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Conv1D(43, (3), padding='same', activation='relu'))\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# # model.add(Conv1D(43, (3), input_shape=(None, 43), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "# model.add(Flatten())\n",
    "# # model.add(Dense(1032, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "# # model.add(Dropout(0.2))\n",
    "# model.add(Dense(516, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(nn_output_dim, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "lrate = 0.1\n",
    "decay = lrate/epochs\n",
    "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Compile model\n",
    "# model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "126160/126160 [==============================] - 12352s - loss: 0.0383 - acc: 0.9515 \n",
      "Epoch 2/3\n",
      "126160/126160 [==============================] - 13935s - loss: 0.0213 - acc: 0.9745 \n",
      "Epoch 3/3\n",
      "126160/126160 [==============================] - 15166s - loss: 0.0193 - acc: 0.9773 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3c6eeb3550>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(X[:429145], y[:429145], epochs=5, batch_size=10)\n",
    "\n",
    "model.fit_generator(generate_inputs_from_file('internal_representations/inputs/inputs_shuffeled.h5', 16),\n",
    "        steps_per_epoch=math.ceil(num_examples/float(batch_size)), epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('cnn_per_vowel_3epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model('cnn_i2_s_c43-3relu_d20_c43-3relu_mp2_f_516relu_d20_121sigmoid_mse_adam_a65.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-8c194ba1d389>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# evaluate the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n%s: %.2f%%\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X[429145:], y[429145:])\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 0]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# calculate predictions\n",
    "# test = generate_input_from_word('test', max_word, dictionary)\n",
    "# print test.shape\n",
    "# print test.T.shape\n",
    "predictions = model.predict(np.array([generate_input_from_word('biotski', max_word, dictionary)]))\n",
    "# round predictions\n",
    "\n",
    "print decode_position(predictions[0], max_num_vowels)\n",
    "\n",
    "b_pred = 0\n",
    "ind = 0\n",
    "i = 0\n",
    "for el in predictions[0]:\n",
    "    if b_pred < el:\n",
    "        b_pred = el\n",
    "        ind = i\n",
    "    i += 1\n",
    "    \n",
    "print(ind)\n",
    "# rounded = [round(x[0]) for x in predictions]\n",
    "# print(rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    }
   ],
   "source": [
    "# calculate predictions\n",
    "# test = generate_input_from_word('test', max_word, dictionary)\n",
    "# print test.shape\n",
    "# print test.T.shape\n",
    "# print generate_input_per_vowel_from_word('biotski', max_word, dictionary, vowels)[0].shape\n",
    "X, X_pure = generate_input_per_vowel_from_word('izziv', max_word, dictionary, vowels)\n",
    "# print X_pure\n",
    "predictions = model.predict([X, X_pure])\n",
    "# round predictions\n",
    "# print predictions\n",
    "# for i in predictions:\n",
    "#     print i[0]\n",
    "\n",
    "\n",
    "print decode_position_from_vowel_to_final_number(predictions)\n",
    "\n",
    "# b_pred = 0\n",
    "# ind = 0\n",
    "# i = 0\n",
    "# for el in predictions[0]:\n",
    "#     if b_pred < el:\n",
    "#         b_pred = el\n",
    "#         ind = i\n",
    "#     i += 1\n",
    "    \n",
    "# print(ind)\n",
    "# rounded = [round(x[0]) for x in predictions]\n",
    "# print(rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "print max_num_vowels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING CONTENT...\n",
      "CONTENT READ SUCCESSFULY\n",
      "CREATING DICTIONARY...\n",
      "DICTIONARY CREATION SUCCESSFUL!\n"
     ]
    }
   ],
   "source": [
    "dictionary, max_word, max_num_vowels, content, vowels, accetuated_vowels = create_dict()\n",
    "def generate_input_from_word(word, max_word, dictionary):\n",
    "    x = np.zeros((max_word, len(dictionary)))\n",
    "    j = 0\n",
    "    for c in list(word):\n",
    "        index = 0\n",
    "        for d in dictionary:\n",
    "            if c == d:\n",
    "                x[j, index] = 1\n",
    "                break\n",
    "            index += 1\n",
    "        j += 1\n",
    "    return x\n",
    "\n",
    "# model = load_model()\n",
    "# prediction = predict(model, generate_input_from_word('hidrija'))\n",
    "# print decode_position(prediction[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
