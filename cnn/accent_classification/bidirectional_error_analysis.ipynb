{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import unicode_literals\n",
    "# text in Western (Windows 1252)\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.models import load_model\n",
    "\n",
    "from copy import copy\n",
    "# from keras import backend as Input\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.path.insert(0, '../../')\n",
    "sys.path.insert(0, '/home/luka/Developement/accetuation/')\n",
    "from prepare_data import *\n",
    "\n",
    "%run ../../prepare_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING DATA...\n",
      "LOAD SUCCESSFUL!\n"
     ]
    }
   ],
   "source": [
    "data = Data('sl', accent_classification=True, reverse_inputs=False)\n",
    "data.generate_data('syllables_accent_classification_correct_input_order_train',\n",
    "                   'syllables_accent_classification_correct_input_order_test',\n",
    "                   'syllables_accent_classification_correct_input_order_validate', content_name='SlovarIJS_BESEDE_utf8.lex',\n",
    "                      content_shuffle_vector='content_shuffle_vector', shuffle_vector='shuffle_vector',\n",
    "                      inputs_location='../internal_representations/inputs/', content_location='../../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING DATA...\n",
      "LOAD SUCCESSFUL!\n"
     ]
    }
   ],
   "source": [
    "data = Data('sl', accent_classification=True)\n",
    "data.generate_data('syllables_accent_classification_train',\n",
    "                   'syllables_accent_classification_test',\n",
    "                   'syllables_accent_classification_validate', content_name='SlovarIJS_BESEDE_utf8.lex',\n",
    "                      content_shuffle_vector='content_shuffle_vector', shuffle_vector='shuffle_vector',\n",
    "                      inputs_location='../internal_representations/inputs/', content_location='../../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING CONTENT...\n",
      "CONTENT READ SUCCESSFULLY\n",
      "CREATING DICTIONARY...\n",
      "DICTIONARY CREATION SUCCESSFUL!\n",
      "GENERATING X AND y...\n",
      "SHUFFELING INPUTS...\n",
      "INPUTS SHUFFELED!\n",
      "SHUFFELING INPUTS...\n",
      "INPUTS SHUFFELED!\n",
      "SHUFFELING INPUTS...\n",
      "INPUTS SHUFFELED!\n",
      "GENERATION SUCCESSFUL!\n"
     ]
    }
   ],
   "source": [
    "data = Data('s', accent_classification=True, reverse_inputs=False)\n",
    "data.generate_data('syllables_accent_classification_correct_input_order_train',\n",
    "                   'syllables_accent_classification_correct_input_order_test',\n",
    "                   'syllables_accent_classification_correct_input_order_validate', content_name='SlovarIJS_BESEDE_utf8.lex',\n",
    "                      content_shuffle_vector='content_shuffle_vector', shuffle_vector='shuffle_vector',\n",
    "                      inputs_location='../internal_representations/inputs/', content_location='../../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING DATA...\n",
      "LOAD SUCCESSFUL!\n"
     ]
    }
   ],
   "source": [
    "data = Data('s', accent_classification=True)\n",
    "data.generate_data('syllables_accent_classification_train',\n",
    "                   'syllables_accent_classification_test',\n",
    "                   'syllables_accent_classification_validate', content_name='SlovarIJS_BESEDE_utf8.lex',\n",
    "                      content_shuffle_vector='content_shuffle_vector', shuffle_vector='shuffle_vector',\n",
    "                      inputs_location='../internal_representations/inputs/', content_location='../../data/', test_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING DATA...\n",
      "LOAD SUCCESSFUL!\n"
     ]
    }
   ],
   "source": [
    "data = Data('l', accent_classification=True, reverse_inputs=False)\n",
    "data.generate_data('letters_accent_classification_correct_input_order_train',\n",
    "                   'letters_accent_classification_correct_input_order_test',\n",
    "                   'letters_accent_classification_correct_input_order_validate',\n",
    "                   inputs_location='../internal_representations/inputs/', content_location='../../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING DATA...\n",
      "LOAD SUCCESSFUL!\n"
     ]
    }
   ],
   "source": [
    "data = Data('l', accent_classification=True)\n",
    "data.generate_data('letters_accent_classification_train',\n",
    "                   'letters_accent_classification_test',\n",
    "                   'letters_accent_classification_validate', content_name='SlovarIJS_BESEDE_utf8.lex',\n",
    "                      content_shuffle_vector='content_shuffle_vector', shuffle_vector='shuffle_vector',\n",
    "                      inputs_location='../internal_representations/inputs/', content_location='../../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data.x_train = None\n",
    "content = data._read_content('../../data/SlovarIJS_BESEDE_utf8.lex')\n",
    "dictionary, max_word, max_num_vowels, vowels, accented_vowels = data._create_dict(content)\n",
    "feature_dictionary = data._create_feature_dictionary()\n",
    "syllable_dictionary = data._create_syllables_dictionary(content, vowels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5168"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(syllable_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.x_train = np.concatenate((data.x_train, data.x_test), axis=0)\n",
    "data.x_other_features_train = np.concatenate((data.x_other_features_train, data.x_other_features_test), axis=0)\n",
    "data.y_train = np.concatenate((data.y_train, data.y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_output_dim = 13\n",
    "nn_hdim = 516\n",
    "batch_size = 16\n",
    "# actual_epoch = 1\n",
    "actual_epoch = 20\n",
    "# num_fake_epoch = 2\n",
    "num_fake_epoch = 20\n",
    "\n",
    "\n",
    "\n",
    "# letters\n",
    "conv_input_shape=(23, 36)\n",
    "\n",
    "# syllabled letters\n",
    "# conv_input_shape=(10, 5168)\n",
    "othr_input = (150, )\n",
    "\n",
    "conv_input = Input(shape=conv_input_shape, name='conv_input')\n",
    "x_conv = Conv1D(115, (3), padding='same', activation='relu')(conv_input)\n",
    "x_conv = Conv1D(46, (3), padding='same', activation='relu')(x_conv)\n",
    "x_conv = MaxPooling1D(pool_size=2)(x_conv)\n",
    "x_conv = Flatten()(x_conv)\n",
    "\n",
    "\n",
    "othr_input = Input(shape=othr_input, name='othr_input')\n",
    "\n",
    "x = concatenate([x_conv, othr_input])\n",
    "# x = Dense(1024, input_dim=(516 + 256), activation='relu')(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(nn_output_dim, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Model(inputs=[conv_input, othr_input], outputs=x)\n",
    "opt = optimizers.Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[actual_accuracy,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('letters/v3_0/20_test_epoch.h5')\n",
    "#model = load_model('syllabled_letters/v2_5_3/40_epoch.h5', custom_objects={'actual_accuracy': actual_accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_output_dim = 13\n",
    "nn_hdim = 516\n",
    "batch_size = 16\n",
    "# actual_epoch = 1\n",
    "actual_epoch = 20\n",
    "# num_fake_epoch = 2\n",
    "num_fake_epoch = 20\n",
    "\n",
    "\n",
    "\n",
    "# letters\n",
    "# conv_input_shape=(23, 36)\n",
    "\n",
    "# syllabled letters\n",
    "conv_input_shape=(10, 5168)\n",
    "\n",
    "\n",
    "othr_input = (150, )\n",
    "\n",
    "conv_input = Input(shape=conv_input_shape, name='conv_input')\n",
    "# letters\n",
    "# x_conv = Conv1D(115, (3), padding='same', activation='relu')(conv_input)\n",
    "# x_conv = Conv1D(46, (3), padding='same', activation='relu')(x_conv)\n",
    "\n",
    "# syllabled letters\n",
    "x_conv = Conv1D(200, (2), padding='same', activation='relu')(conv_input)\n",
    "x_conv = MaxPooling1D(pool_size=2)(x_conv)\n",
    "x_conv = Flatten()(x_conv)\n",
    "\n",
    "othr_input = Input(shape=othr_input, name='othr_input')\n",
    "\n",
    "x = concatenate([x_conv, othr_input])\n",
    "# x = Dense(1024, input_dim=(516 + 256), activation='relu')(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(nn_output_dim, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Model(inputs=[conv_input, othr_input], outputs=x)\n",
    "opt = optimizers.Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[actual_accuracy,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.load_weights('syllables/v2_0/20_test_epoch.h5')\n",
    "model.load_weights('syllables/v2_2/20_test_epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_output_dim = 13\n",
    "nn_hdim = 516\n",
    "batch_size = 16\n",
    "# actual_epoch = 1\n",
    "actual_epoch = 20\n",
    "# num_fake_epoch = 2\n",
    "num_fake_epoch = 20\n",
    "\n",
    "\n",
    "\n",
    "# letters\n",
    "# conv_input_shape=(23, 36)\n",
    "\n",
    "# syllabled letters\n",
    "conv_input_shape=(10, 252)\n",
    "\n",
    "\n",
    "othr_input = (150, )\n",
    "\n",
    "conv_input = Input(shape=conv_input_shape, name='conv_input')\n",
    "# letters\n",
    "# x_conv = Conv1D(115, (3), padding='same', activation='relu')(conv_input)\n",
    "# x_conv = Conv1D(46, (3), padding='same', activation='relu')(x_conv)\n",
    "\n",
    "# syllabled letters\n",
    "x_conv = Conv1D(200, (2), padding='same', activation='relu')(conv_input)\n",
    "x_conv = MaxPooling1D(pool_size=2)(x_conv)\n",
    "x_conv = Flatten()(x_conv)\n",
    "\n",
    "othr_input = Input(shape=othr_input, name='othr_input')\n",
    "\n",
    "x = concatenate([x_conv, othr_input])\n",
    "# x = Dense(1024, input_dim=(516 + 256), activation='relu')(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(nn_output_dim, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Model(inputs=[conv_input, othr_input], outputs=x)\n",
    "opt = optimizers.Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[actual_accuracy,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('syllabled_letters/v2_2/20_test_epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#95.54408764380132\n",
    "#95.31371030209141\n",
    "data._reverse_inputs = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_array = np.asarray(data.y_validate)\n",
    "accentuation_length = (y_array > 0).sum()\n",
    "predictions = model.predict_generator(data.generator('validate', batch_size, content_name='SlovarIJS_BESEDE_utf8.lex', content_location='../../data/'), \n",
    "                                      accentuation_length/(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58327\n",
      "54222\n",
      "51745\n",
      "2477\n",
      "54222\n",
      "95.6589572582166\n",
      "95.43174357271957\n"
     ]
    }
   ],
   "source": [
    "accuracy, real_accuracy, errors = data.test_type_accuracy(predictions, data.x_validate, data.x_other_features_validate, data.y_validate, dictionary, feature_dictionary, vowels, accented_vowels, syllable_dictionary=syllable_dictionary)\n",
    "print(accuracy)\n",
    "print(real_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58327, 13)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reorder_correct_direction_inputs(predictions, y):\n",
    "    pred_i = 0\n",
    "    for i in range(len(y)):\n",
    "        num_accented_syllables = 0\n",
    "        for el in y[i]:\n",
    "            if el > 0:\n",
    "                num_accented_syllables += 1\n",
    "        if num_accented_syllables > 1:\n",
    "            min_i = pred_i\n",
    "            max_i = pred_i + num_accented_syllables - 1\n",
    "            while(max_i > min_i):\n",
    "                min_pred = copy(predictions[min_i])\n",
    "                max_pred = copy(predictions[max_i])\n",
    "                predictions[min_i] = max_pred\n",
    "                predictions[max_i] = min_pred\n",
    "                min_i += 1\n",
    "                max_i -= 1\n",
    "        pred_i += num_accented_syllables\n",
    "        \n",
    "reorder_correct_direction_inputs(predictions, data.y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = open('error_analysis_results/onedirectional_input/syllabled_letters_accent_classification_co_oversampling_error.pkl', 'wb')\n",
    "pickle.dump(errors, output)\n",
    "output.close()\n",
    "output = open('error_analysis_results/onedirectional_input/syllabled_letters_accent_classification_co_oversampling_predictions.pkl', 'wb')\n",
    "pickle.dump(predictions, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_input = open('error_analysis_results/onedirectional_input/letters_accent_classification_correct_order_reversed_predictions.pkl', 'rb')\n",
    "letters_accent_classification_correct_order_reversed_predictions = pickle.load(pickle_input)\n",
    "pickle_input = open('error_analysis_results/onedirectional_input/syllables_accent_classification_correct_order_reversed_predictions.pkl', 'rb')\n",
    "syllables_accent_classification_correct_order_reversed_predictions = pickle.load(pickle_input)\n",
    "pickle_input = open('error_analysis_results/onedirectional_input/syllabled_letters_accent_classification_correct_order_reversed_predictions.pkl', 'rb')\n",
    "syllabled_letters_accent_classification_correct_order_reversed_predictions = pickle.load(pickle_input)\n",
    "\n",
    "pickle_input = open('letters_accent_classification_test_predictions.pkl', 'rb')\n",
    "letters_accent_classification_predictions = pickle.load(pickle_input)\n",
    "pickle_input = open('syllables_accent_classification_test_predictions.pkl', 'rb')\n",
    "syllables_accent_classification_predictions = pickle.load(pickle_input)\n",
    "pickle_input = open('syllabled_letters_accent_classification_test_predictions.pkl', 'rb')\n",
    "syllabled_letters_accent_classification_predictions = pickle.load(pickle_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58327\n",
      "54222\n",
      "2837\n",
      "51385\n",
      "54222\n",
      "3.228350506626434\n",
      "5.23219357456383\n"
     ]
    }
   ],
   "source": [
    "ensemble_predictions = np.mean( np.array([ letters_accent_classification_predictions, syllables_accent_classification_predictions, syllabled_letters_accent_classification_predictions,\n",
    "                                         letters_accent_classification_correct_order_reversed_predictions, syllables_accent_classification_correct_order_reversed_predictions, syllabled_letters_accent_classification_correct_order_reversed_predictions]), axis=0 )\n",
    "\n",
    "#ensemble_predictions = np.mean( np.array([ letters_accent_classification_predictions, syllabled_letters_accent_classification_predictions,\n",
    "#                                         letters_accent_classification_correct_order_reversed_predictions, syllabled_letters_accent_classification_correct_order_reversed_predictions]), axis=0 )\n",
    "\n",
    "#ensemble_predictions = np.mean( np.array([ letters_accent_classification_predictions, syllables_accent_classification_predictions, syllabled_letters_accent_classification_predictions ]), axis=0 )\n",
    "accuracy, real_accuracy, errors = data.test_type_accuracy(ensemble_accent_classification_predictions, data.x_validate, data.x_other_features_validate, data.y_validate, dictionary, feature_dictionary, vowels, accented_vowels, syllable_dictionary=syllable_dictionary)\n",
    "# accuracy, real_accuracy, errors = data.test_type_accuracy(ensemble_predictions, data.x_validate, data.x_other_features_validate, data.y_validate, dictionary, feature_dictionary, vowels, accented_vowels, syllable_dictionary=syllable_dictionary)\n",
    "print(accuracy)\n",
    "print(real_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[  0.00000000e+00   1.00000000e+00   4.25894693e-12   1.31152606e-21\n",
      "   7.07428871e-37   6.76180402e-29   6.22833575e-15   0.00000000e+00\n",
      "   5.17264047e-22   0.00000000e+00   0.00000000e+00   2.81393113e-18\n",
      "   0.00000000e+00]\n",
      "[  0.00000000e+00   0.00000000e+00   0.00000000e+00   1.51565325e-25\n",
      "   1.00000000e+00   1.61675484e-34   6.17491277e-39   0.00000000e+00\n",
      "   2.99232293e-38   0.00000000e+00   4.22588520e-34   0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "[  0.00000000e+00   9.99982595e-01   9.06433911e-07   3.11275741e-16\n",
      "   3.69056970e-18   1.20418770e-17   1.11667652e-12   2.57631544e-25\n",
      "   1.55146012e-17   0.00000000e+00   2.08245277e-24   0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "[  0.00000000e+00   2.67856488e-26   4.35386780e-13   1.66966936e-17\n",
      "   1.00000000e+00   1.27324307e-27   6.43282027e-28   5.31304462e-32\n",
      "   1.08525564e-34   0.00000000e+00   7.42423605e-29   0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "95.5036700970086\n"
     ]
    }
   ],
   "source": [
    "print(data.y_validate[25])\n",
    "print(predictions[25])\n",
    "print(predictions[26])\n",
    "print(letters_accent_classification_predictions[25])\n",
    "print(letters_accent_classification_predictions[26])\n",
    "print(real_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[  0.00000000e+00   0.00000000e+00   0.00000000e+00   1.51565325e-25\n",
      "   1.00000000e+00   1.61675484e-34   6.17491277e-39   0.00000000e+00\n",
      "   2.99232293e-38   0.00000000e+00   4.22588520e-34   0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "[  0.00000000e+00   1.00000000e+00   4.25894693e-12   1.31152606e-21\n",
      "   7.07428871e-37   6.76180402e-29   6.22833575e-15   0.00000000e+00\n",
      "   5.17264047e-22   0.00000000e+00   0.00000000e+00   2.81393113e-18\n",
      "   0.00000000e+00]\n",
      "[  0.00000000e+00   9.99982595e-01   9.06433911e-07   3.11275741e-16\n",
      "   3.69056970e-18   1.20418770e-17   1.11667652e-12   2.57631544e-25\n",
      "   1.55146012e-17   0.00000000e+00   2.08245277e-24   0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "[  0.00000000e+00   2.67856488e-26   4.35386780e-13   1.66966936e-17\n",
      "   1.00000000e+00   1.27324307e-27   6.43282027e-28   5.31304462e-32\n",
      "   1.08525564e-34   0.00000000e+00   7.42423605e-29   0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "95.5036700970086\n"
     ]
    }
   ],
   "source": [
    "print(data.y_validate[25])\n",
    "print(predictions[25])\n",
    "print(predictions[26])\n",
    "print(letters_accent_classification_predictions[25])\n",
    "print(letters_accent_classification_predictions[26])\n",
    "print(real_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.9573235955885\n"
     ]
    }
   ],
   "source": [
    "accuracy, errors = data.test_accuracy(new_predictions, data.x_validate, data.x_other_features_validate, data.y_validate, dictionary, feature_dictionary, vowels, syllable_dictionary=syllable_dictionary)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.30016403e-07,   9.99997973e-01,   2.21242431e-14,\n",
       "         4.04031276e-34,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.30016403e-07,   9.99997973e-01,   2.21242431e-14,\n",
       "         4.04031276e-34,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.21242431e-14,   9.99997973e-01,   1.30016403e-07,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_predictions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.73564101579008\n"
     ]
    }
   ],
   "source": [
    "def get_word_length(x_el):\n",
    "    i = 0\n",
    "    for el in x_el:\n",
    "        if el == 0:\n",
    "            return i\n",
    "        i += 1\n",
    "\n",
    "new_predictions = np.zeros(predictions.shape, dtype='float32')\n",
    "for i in range(len(predictions)):\n",
    "    word_len = get_word_length(data.x_test[i])\n",
    "    #print(word_len)\n",
    "    #if word_len > 6:\n",
    "    #    print(word_len)\n",
    "    #    print(data.y_test[i])\n",
    "    \n",
    "    #\n",
    "    for k in range(word_len):\n",
    "        new_predictions[i][k] += predictions[i][word_len - 1 - k]\n",
    "\n",
    "        \n",
    "accuracy, errors = data.test_accuracy(new_predictions, data.x_test, data.x_other_features_test, data.y_test, dictionary, feature_dictionary, vowels, syllable_dictionary=syllable_dictionary)\n",
    "print(accuracy)\n",
    "        \n",
    "errors = [[el[0], el[1][::-1], el[2], el[3][::-1], el[4][::-1]] for el in errors]\n",
    "\n",
    "\n",
    "errors.sort(key=lambda x: x[1])\n",
    "# name = 'error_analysis_results/bidirectional_input/letters_word_accetuation'\n",
    "# name = 'error_analysis_results/bidirectional_input/letters_word_accetuation_test'\n",
    "# name = 'error_analysis_results/bidirectional_input/syllables_word_accetuation'\n",
    "# name = 'error_analysis_results/bidirectional_input/syllables_word_accetuation_test'\n",
    "# name = 'error_analysis_results/bidirectional_input/syllabled_letters_word_accetuation'\n",
    "#name = 'error_analysis_results/bidirectional_input/syllabled_letters_word_accetuation_test'\n",
    "# name = 'error_analysis_results/onedirectional_input/letters_word_accetuation_correct_order_reversed_test'\n",
    "# name = 'error_analysis_results/onedirectional_input/syllabled_letters_word_accetuation_correct_order_reversed_test'\n",
    "# name = 'error_analysis_results/onedirectional_input/syllables_word_accetuation_correct_order_reversed_test'\n",
    "# name = 'error_analysis_results/onedirectional_input/letters_word_accetuation_correct_order_reversed'\n",
    "# name = 'error_analysis_results/onedirectional_input/syllabled_letters_word_accetuation_correct_order_reversed'\n",
    "# name = 'error_analysis_results/onedirectional_input/syllables_word_accetuation_correct_order_reversed'\n",
    "output = open(name + '_error.pkl', 'wb')\n",
    "pickle.dump(errors, output)\n",
    "output.close()\n",
    "output = open(name + '_predictions.pkl', 'wb')\n",
    "pickle.dump(new_predictions, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEST DATA\n",
    "#pickle_input = open('letters_word_accetuation_test_error.pkl', 'rb')\n",
    "#letters_word_accetuation_test = pickle.load(pickle_input)\n",
    "#pickle_input = open('syllables_word_accetuation_test_error.pkl', 'rb')\n",
    "#syllables_word_accetuation_test = pickle.load(pickle_input)\n",
    "#pickle_input = open('syllabled_letters_word_accetuation_test_error.pkl', 'rb')\n",
    "#syllabled_letters_word_accetuation_test = pickle.load(pickle_input)\n",
    "#pickle_input = open('ensemble_test_errors.pkl', 'rb')\n",
    "#ensemble_word_accetuation_test = pickle.load(pickle_input)\n",
    "\n",
    "pickle_input = open('letters_word_accetuation_predictions.pkl', 'rb')\n",
    "letters_word_accetuation_predictions = pickle.load(pickle_input)\n",
    "pickle_input = open('syllables_word_accetuation_predictions.pkl', 'rb')\n",
    "syllables_word_accetuation_predictions = pickle.load(pickle_input)\n",
    "pickle_input = open('syllabled_letters_word_accetuation_predictions.pkl', 'rb')\n",
    "syllabled_letters_word_accetuation_predictions = pickle.load(pickle_input)\n",
    "\n",
    "\n",
    "pickle_input = open('error_analysis_results/onedirectional_input/letters_word_accetuation_correct_order_reversed_predictions.pkl', 'rb')\n",
    "letters_word_accetuation_correct_order_predictions = pickle.load(pickle_input)\n",
    "pickle_input = open('error_analysis_results/onedirectional_input/syllabled_letters_word_accetuation_correct_order_reversed_predictions.pkl', 'rb')\n",
    "syllables_word_accetuation_correct_order_predictions = pickle.load(pickle_input)\n",
    "pickle_input = open('error_analysis_results/onedirectional_input/syllables_word_accetuation_correct_order_reversed_predictions.pkl', 'rb')\n",
    "syllabled_letters_word_accetuation_correct_order_predictions = pickle.load(pickle_input)\n",
    "\n",
    "#pickle_input = open('svm/rf_predictions.pkl', 'rb')\n",
    "#rf_predictions = pickle.load(pickle_input)\n",
    "\n",
    "\n",
    "# !!!!!!!!! TEST\n",
    "#pickle_input = open('letters_word_accetuation_test_predictions.pkl', 'rb')\n",
    "#letters_word_accetuation_predictions = pickle.load(pickle_input)\n",
    "#pickle_input = open('syllables_word_accetuation_test_predictions.pkl', 'rb')\n",
    "#syllables_word_accetuation_predictions = pickle.load(pickle_input)\n",
    "#pickle_input = open('syllabled_letters_word_accetuation_test_predictions.pkl', 'rb')\n",
    "#syllabled_letters_word_accetuation_predictions = pickle.load(pickle_input)\n",
    "\n",
    "\n",
    "#pickle_input = open('error_analysis_results/onedirectional_input/letters_word_accetuation_correct_order_reversed_test_predictions.pkl', 'rb')\n",
    "#letters_word_accetuation_correct_order_predictions = pickle.load(pickle_input)\n",
    "#pickle_input = open('error_analysis_results/onedirectional_input/syllabled_letters_word_accetuation_correct_order_reversed_test_predictions.pkl', 'rb')\n",
    "#syllables_word_accetuation_correct_order_predictions = pickle.load(pickle_input)\n",
    "#pickle_input = open('error_analysis_results/onedirectional_input/syllables_word_accetuation_correct_order_reversed_test_predictions.pkl', 'rb')\n",
    "#syllabled_letters_word_accetuation_correct_order_predictions = pickle.load(pickle_input)\n",
    "\n",
    "#pickle_input = open('svm/rf_predictions.pkl', 'rb')\n",
    "#rf_predictions = pickle.load(pickle_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.21041914787352\n"
     ]
    }
   ],
   "source": [
    "ensemble_predictions = np.mean( np.array([ letters_word_accetuation_predictions, syllables_word_accetuation_predictions, syllabled_letters_word_accetuation_predictions, \n",
    "                                         letters_word_accetuation_correct_order_predictions, syllables_word_accetuation_correct_order_predictions, syllabled_letters_word_accetuation_correct_order_predictions]), axis=0 )\n",
    "#ensemble_predictions = np.mean( np.array([ letters_word_accetuation_predictions, syllables_word_accetuation_predictions, syllabled_letters_word_accetuation_predictions]), axis=0 )\n",
    "#accuracy, errors = data.test_accuracy(ensemble_predictions, data.x_validate, data.x_other_features_validate, data.y_validate, dictionary, feature_dictionary, vowels, syllable_dictionary=syllable_dictionary)\n",
    "ensemble_predictions=syllabled_letters_word_accetuation_correct_order_predictions\n",
    "accuracy, errors = data.test_accuracy(ensemble_predictions, data.x_test, data.x_other_features_test, data.y_test, dictionary, feature_dictionary, vowels, syllable_dictionary=syllable_dictionary, threshold=0.5)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.62779976180414\n"
     ]
    }
   ],
   "source": [
    "ensemble_predictions = np.mean( np.array([ letters_word_accetuation_predictions, syllables_word_accetuation_predictions, syllabled_letters_word_accetuation_predictions, \n",
    "                                         letters_word_accetuation_correct_order_predictions, syllables_word_accetuation_correct_order_predictions, syllabled_letters_word_accetuation_correct_order_predictions]), axis=0 )\n",
    "#ensemble_predictions = np.mean( np.array([ letters_word_accetuation_predictions, syllables_word_accetuation_predictions, syllabled_letters_word_accetuation_predictions]), axis=0 )\n",
    "#accuracy, errors = data.test_accuracy(ensemble_predictions, data.x_validate, data.x_other_features_validate, data.y_validate, dictionary, feature_dictionary, vowels, syllable_dictionary=syllable_dictionary)\n",
    "accuracy, errors = data.test_accuracy(ensemble_predictions, data.x_test, data.x_other_features_test, data.y_test, dictionary, feature_dictionary, vowels, syllable_dictionary=syllable_dictionary, threshold=0.1)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy, errors = data.test_accuracy(ensemble_predictions, data.x_test, data.x_other_features_test, data.y_test, dictionary, feature_dictionary, vowels, syllable_dictionary=syllable_dictionary, threshold=0.1)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6609331e-10"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_predictions[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "errors = [[el[0], el[1][::-1], el[2], el[3][::-1], el[4][::-1]] for el in errors]\n",
    "errors.sort(key=lambda x: x[1])\n",
    "\n",
    "output = open('error_analysis_results/onedirectional_input/ensemble_errors.pkl', 'wb')\n",
    "pickle.dump(errors, output)\n",
    "output.close()\n",
    "output = open('error_analysis_results/onedirectional_input/ensemble_predictions.pkl', 'wb')\n",
    "pickle.dump(ensemble_predictions, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52058"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6214\n",
      "7770\n",
      "6515\n",
      "3232\n",
      "3662\n",
      "3868\n",
      "0.8641323139575089\n",
      "2609\n",
      "UNIQUE ERRORS\n",
      "1594\n"
     ]
    }
   ],
   "source": [
    "def compare_outputs(outputs1, outputs2):\n",
    "    outputs2_indices = [el[0] for el in outputs2]\n",
    "    return [el for el in outputs1 if el[0] in outputs2_indices]\n",
    "\n",
    "def get_unique_errors(outputs1, outputs2):\n",
    "    outputs2_indices = [el[0] for el in outputs2]\n",
    "    return [el for el in outputs1 if el[0] not in outputs2_indices]\n",
    "\n",
    "print(len(letters_word_accetuation))\n",
    "print(len(syllables_word_accetuation))\n",
    "print(len(syllabled_letters_word_accetuation))\n",
    "print(len(compare_outputs(letters_word_accetuation, syllables_word_accetuation)))\n",
    "print(len(compare_outputs(letters_word_accetuation, syllabled_letters_word_accetuation)))\n",
    "print(len(compare_outputs(syllables_word_accetuation, syllabled_letters_word_accetuation)))\n",
    "\n",
    "\n",
    "print(1  - 7073.0/52058)\n",
    "collective_errors = compare_outputs(letters_word_accetuation, compare_outputs(syllabled_letters_word_accetuation, syllables_word_accetuation))\n",
    "print(len(collective_errors))\n",
    "\n",
    "# print(letters_word_accetuation[:20])\n",
    "# print(syllables_word_accetuation[:20])\n",
    "# print(syllabled_letters_word_accetuation[:20])\n",
    "# print(collective_errors)\n",
    "\n",
    "# unique_err = get_unique_errors(get_unique_errors(letters_word_accetuation, syllables_word_accetuation), syllabled_letters_word_accetuation)\n",
    "# unique_err = get_unique_errors(get_unique_errors(syllables_word_accetuation, letters_word_accetuation), syllabled_letters_word_accetuation)\n",
    "unique_err = get_unique_errors(get_unique_errors(syllabled_letters_word_accetuation, letters_word_accetuation), syllables_word_accetuation)\n",
    "print(\"UNIQUE ERRORS\")\n",
    "print(len(unique_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.84468085e-08   9.99999106e-01   2.88171825e-17   7.48493946e-24\n",
      "   4.87220418e-36   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "[  3.23229117e-11   9.99989212e-01   2.31212027e-08   2.49620165e-26\n",
      "   3.08989440e-20   4.69045294e-21   4.73113282e-38   1.19897549e-33\n",
      "   8.32371040e-34   0.00000000e+00]\n",
      "[  4.11718755e-27   1.00000000e+00   2.89068776e-27   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "[  6.15971052e-09   9.99996126e-01   7.70706787e-09   2.50330040e-24\n",
      "   1.02996483e-20   1.56348435e-21   1.57704427e-38   3.99658482e-34\n",
      "   2.77457006e-34   0.00000000e+00]\n",
      "0.9999961059999999\n"
     ]
    }
   ],
   "source": [
    "print(letters_word_accetuation_predictions[0])\n",
    "print(syllables_word_accetuation_predictions[0])\n",
    "print(syllabled_letters_word_accetuation_predictions[0])\n",
    "\n",
    "test = np.mean( np.array([ letters_word_accetuation_predictions, syllables_word_accetuation_predictions, syllabled_letters_word_accetuation_predictions ]), axis=0 )\n",
    "print(test[0])\n",
    "print((9.99999106e-01 + 9.99989212e-01 + 1.00000000e+00)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.7054055092397\n"
     ]
    }
   ],
   "source": [
    "ensemble_predictions = np.mean( np.array([ letters_word_accetuation_predictions, syllables_word_accetuation_predictions, syllabled_letters_word_accetuation_predictions ]), axis=0 )\n",
    "accuracy, errors = data.test_accuracy(ensemble_predictions, data.x_test, data.x_other_features_test, data.y_test, dictionary, feature_dictionary, vowels, syllable_dictionary=syllable_dictionary)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.7054055092397"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_input = open('error_analysis_results/onedirectional_input/ensemble_predictions.pkl', 'rb')\n",
    "ensemble_accent_classification_predictions = pickle.load(pickle_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_input = open('../accent_classification/syllabled_letters_accent_classification_test_predictions.pkl', 'rb')\n",
    "syllables_word_accetuation_predictions = pickle.load(pickle_input)\n",
    "ensemble = syllables_word_accetuation_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_input = open('error_analysis_results/onedirectional_input/syllabled_letters_accent_classification_co_oversampling_predictions.pkl', 'rb')\n",
    "syllables_word_accetuation_predictions = pickle.load(pickle_input)\n",
    "ensemble = syllables_word_accetuation_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58327\n",
      "54222\n",
      "51745\n",
      "2477\n",
      "54222\n"
     ]
    }
   ],
   "source": [
    "accuracy, real_accuracy, errors = data.test_type_accuracy(ensemble, data.x_validate, data.x_other_features_validate, data.y_validate, dictionary, feature_dictionary, vowels, accented_vowels, syllable_dictionary=syllable_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.98318025893549"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.43174357271957"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "probabilities = np.zeros((13, 13))\n",
    "count_results = np.zeros(13)\n",
    "l = 0\n",
    "max_indices = []\n",
    "for i in range(len(data.y_validate)):\n",
    "    decoded_x = data.decode_syllable_x(data.x_validate[i], syllable_dictionary)\n",
    "    for j in range(10):\n",
    "        if data.y_validate[i][j] > 0.0:\n",
    "            \n",
    "            \n",
    "            stressed_letter = data.get_accentuated_letter(decoded_x, j, vowels, syllables=data._input_type != 'l')\n",
    "            possible_places = np.zeros(len(ensemble[l]))\n",
    "            if stressed_letter == 'r':\n",
    "                possible_places[0] = 1\n",
    "            elif stressed_letter == 'a':\n",
    "                possible_places[1] = 1\n",
    "                possible_places[2] = 1\n",
    "            elif stressed_letter == 'e':\n",
    "                possible_places[3] = 1\n",
    "                possible_places[4] = 1\n",
    "                possible_places[5] = 1\n",
    "            elif stressed_letter == 'i':\n",
    "                possible_places[6] = 1\n",
    "                possible_places[7] = 1\n",
    "            elif stressed_letter == 'o':\n",
    "                possible_places[8] = 1\n",
    "                possible_places[9] = 1\n",
    "                possible_places[10] = 1\n",
    "            elif stressed_letter == 'u':\n",
    "                possible_places[11] = 1\n",
    "                possible_places[12] = 1\n",
    "            possible_predictions = ensemble[l] * possible_places\n",
    "            \n",
    "            \n",
    "            max_index = np.argmax(possible_predictions)\n",
    "            possible_predictions[max_index] = 0.0\n",
    "            probabilities[int(data.y_validate[i][j])] += possible_predictions\n",
    "            count_results[int(data.y_validate[i][j])] += 1\n",
    "            l += 1\n",
    "            \n",
    "for i in range(len(probabilities)):\n",
    "    probabilities[i] = probabilities[i] / count_results[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ensemble_accent_classification_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(possible_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "    nan]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.01  0.    0.01  0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.02  0.01  0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.06  0.03  0.11  0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]]\n",
      "[    0 13604  2666 11153  2646  3932 12552   145  7115  1124    48  3336\n",
      "     6]\n"
     ]
    }
   ],
   "source": [
    "print(np.around(probabilities, decimals=2))\n",
    "print(count_results.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "    nan]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.01  0.    0.01  0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.01  0.01  0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.02  0.02  0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.01  0.01  0.03  0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.02\n",
      "   0.05]]\n",
      "[    0 13604  2666 11153  2646  3932 12552   145  7115  1124    48  3336\n",
      "     6]\n"
     ]
    }
   ],
   "source": [
    "print(np.around(probabilities, decimals=2))\n",
    "print(count_results.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "    nan]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.01  0.    0.01  0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.02  0.01  0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.06  0.03  0.11  0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]]\n",
      "[    0 13604  2666 11153  2646  3932 12552   145  7115  1124    48  3336\n",
      "     6]\n"
     ]
    }
   ],
   "source": [
    "print(np.around(probabilities, decimals=2))\n",
    "print(count_results.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '', '', '', '', '', '', '', '', '', '', '']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data._get_accented_vowels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1359  834    0    0    0    0    0    0    0    0]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(data.x_test[0])\n",
    "print(get_word_length(data.x_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "# COLLECTIVE RESULTS\n",
    "#######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEST DATA!!!\n",
    "pickle_input = open('../accent_classification/letters_accent_classification_test_predictions.pkl', 'rb')\n",
    "letters_accent_classification_predictions = pickle.load(pickle_input)\n",
    "pickle_input = open('../accent_classification/syllables_accent_classification_test_predictions.pkl', 'rb')\n",
    "syllables_accent_classification_predictions = pickle.load(pickle_input)\n",
    "pickle_input = open('../accent_classification/syllabled_letters_accent_classification_test_predictions.pkl', 'rb')\n",
    "syllabled_letters_accent_classification_predictions = pickle.load(pickle_input)\n",
    "pickle_input = open('../accent_classification/ensemble_test_predictions.pkl', 'rb')\n",
    "ensemble_accent_classification_predictions = pickle.load(pickle_input)\n",
    "\n",
    "pickle_input = open('../accent_classification/letters_accent_classification_test_errors.pkl', 'rb')\n",
    "letters_accent_classification_errors = pickle.load(pickle_input)\n",
    "pickle_input = open('../accent_classification/syllables_accent_classification_test_errors.pkl', 'rb')\n",
    "syllables_accent_classification_errors = pickle.load(pickle_input)\n",
    "pickle_input = open('../accent_classification/syllabled_letters_accent_classification_test_errors.pkl', 'rb')\n",
    "syllabled_letters_accent_classification_errors = pickle.load(pickle_input)\n",
    "pickle_input = open('../accent_classification/ensemble_test_errors.pkl', 'rb')\n",
    "ensemble_accent_classification_errors = pickle.load(pickle_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'letters_word_accetuation_test_error.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c827de19b6de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TEST DATA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpickle_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'letters_word_accetuation_test_error.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mletters_word_accetuation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpickle_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'syllables_word_accetuation_test_error.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msyllables_word_accetuation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'letters_word_accetuation_test_error.pkl'"
     ]
    }
   ],
   "source": [
    "# TEST DATA\n",
    "pickle_input = open('letters_word_accetuation_test_error.pkl', 'rb')\n",
    "letters_word_accetuation = pickle.load(pickle_input)\n",
    "pickle_input = open('syllables_word_accetuation_test_error.pkl', 'rb')\n",
    "syllables_word_accetuation = pickle.load(pickle_input)\n",
    "pickle_input = open('syllabled_letters_word_accetuation_test_error.pkl', 'rb')\n",
    "syllabled_letters_word_accetuation = pickle.load(pickle_input)\n",
    "pickle_input = open('ensemble_test_errors.pkl', 'rb')\n",
    "ensemble_word_accetuation = pickle.load(pickle_input)\n",
    "\n",
    "pickle_input = open('letters_word_accetuation_test_predictions.pkl', 'rb')\n",
    "letters_word_accetuation_predictions = pickle.load(pickle_input)\n",
    "pickle_input = open('syllables_accent_classification_test_predictions.pkl', 'rb')\n",
    "syllables_word_accetuation_predictions = pickle.load(pickle_input)\n",
    "pickle_input = open('syllabled_letters_word_accetuation_test_predictions.pkl', 'rb')\n",
    "syllabled_letters_word_accetuation_predictions = pickle.load(pickle_input)\n",
    "pickle_input = open('ensemble_test_predictions.pkl', 'rb')\n",
    "ensemble_word_accetuation_predictions = pickle.load(pickle_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9637232119803769"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - len(ensemble_accent_classification_errors)/float(data.x_validate.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "letters\n",
      "0.8853970712994725\n",
      "0.9606248386263878\n",
      "0.8564789199955738\n",
      "syllables\n",
      "0.8567002323780015\n",
      "0.9399690162664601\n",
      "0.8071262587141751\n",
      "syllabled letters\n",
      "0.879845819040242\n",
      "0.9598318025893549\n",
      "0.8501346316993103\n",
      "ensembles\n",
      "0.9023643539522703\n",
      "0.9637232119803769\n",
      "0.8765445760023607\n"
     ]
    }
   ],
   "source": [
    "def prediction_accuracy(stress_location_errors, stress_type_errors, length):\n",
    "    print(1 - len(stress_location_errors)/float(length))\n",
    "    print(1 - len(stress_type_errors)/float(length))\n",
    "    collective_error_num = len(stress_location_errors) + len(stress_type_errors) -\\\n",
    "                            len(compare_outputs(letters_accent_classification_errors, letters_word_accetuation_test))\n",
    "    print(1 - collective_error_num/float(length))\n",
    "#print(len(letters_accent_classification_errors))\n",
    "#print(len(letters_word_accetuation_test))\n",
    "#len(compare_outputs(letters_accent_classification_errors, letters_word_accetuation_test))\n",
    "print(\"letters\")\n",
    "prediction_accuracy(letters_word_accetuation, letters_accent_classification_errors, data.x_validate.shape[0])\n",
    "\n",
    "print(\"syllables\")\n",
    "prediction_accuracy(syllables_word_accetuation, syllables_accent_classification_errors, data.x_validate.shape[0])\n",
    "\n",
    "print(\"syllabled letters\")\n",
    "prediction_accuracy(syllabled_letters_word_accetuation, syllabled_letters_accent_classification_errors, data.x_validate.shape[0])\n",
    "\n",
    "print(\"ensembles\")\n",
    "prediction_accuracy(ensemble_word_accetuation, ensemble_accent_classification_errors, data.x_validate.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train\n",
    "y_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37251.0\n",
      "5360.0\n",
      "191.0\n",
      "4553.0\n",
      "236.0\n",
      "1724.0\n",
      "4777.0\n",
      "20.0\n",
      "2688.0\n",
      "481.0\n",
      "2.0\n",
      "1314.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "count_accents = np.zeros(13)\n",
    "for i in range(data.y_validate.shape[0]):\n",
    "    for j in range(data.y_validate.shape[1]):\n",
    "        if data.y_train[i][j] > 0.0:\n",
    "            #print('')\n",
    "            count_accents[int(data.y_validate[i][j])] += 1\n",
    "for el in count_accents:\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58597.0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(count_accents)\n",
    "#data.y_validate.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54222"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y_test.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
