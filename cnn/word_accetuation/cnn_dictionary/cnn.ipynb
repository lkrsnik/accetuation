{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import unicode_literals\n",
    "# text in Western (Windows 1252)\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "# import StringIO\n",
    "import math\n",
    "from keras import optimizers, metrics\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.layers.merge import concatenate\n",
    "from keras import regularizers\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import load_model\n",
    "# from keras import backend as Input\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %run ../../../prepare_data.py\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../../')\n",
    "from prepare_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run ../../../prepare_data.py\n",
    "# X_train, X_other_features_train, y_train, X_test, X_other_features_test, y_test, X_validate, X_other_features_validate, y_validate = generate_full_matrix_inputs('../../internal_representations/inputs/content_shuffle_vector.h5', '../../internal_representations/inputs/shuffle_vector')\n",
    "# save_inputs('../../internal_representations/inputs/shuffeled_matrix_train_inputs_other_features_multilabel.h5', X_train, y_train, other_features = X_other_features_train)\n",
    "# save_inputs('../../internal_representations/inputs/shuffeled_matrix_test_inputs_other_features_multilabel.h5', X_test, y_test, other_features = X_other_features_test)\n",
    "# save_inputs('../../internal_representations/inputs/shuffeled_matrix_validate_inputs_other_features_multilabel.h5', X_validate, y_validate,  other_features = X_other_features_validate)\n",
    "X_train, X_other_features_train, y_train = load_inputs('../../internal_representations/inputs/shuffeled_matrix_train_inputs_other_features_multilabel.h5', other_features=True)\n",
    "X_test, X_other_features_test, y_test = load_inputs('../../internal_representations/inputs/shuffeled_matrix_test_inputs_other_features_multilabel.h5', other_features=True)\n",
    "X_validate, X_other_features_validate, y_validate = load_inputs('../../internal_representations/inputs/shuffeled_matrix_validate_inputs_other_features_multilabel.h5', other_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(430151, 23, 30)\n",
      "(52058, 23, 30)\n",
      "(54222, 23, 30)\n",
      "občutena\n",
      "Agpnpa-\n",
      "Agpnpa-\n"
     ]
    }
   ],
   "source": [
    "print (X_train.shape)\n",
    "print (X_test.shape)\n",
    "print (X_validate.shape)\n",
    "# X_other_features = create_X_features(content)\n",
    "# print (X_other_features[178200])\n",
    "decode_position = 13\n",
    "print (decode_input(X_train[decode_position], dictionary))\n",
    "# print (X_other_features_train[0])\n",
    "print (decode_X_features(feature_dictionary, [X_other_features_train[decode_position]]))\n",
    "# print (len(X_other_features_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_examples = len(X_train) # training set size\n",
    "# nn_input_dim = max_word * len(dictionary) # input layer dimensionality\n",
    "# nn_output_dim = max_num_vowels * max_num_vowels # output layer dimensionality\n",
    "nn_output_dim = 11\n",
    "nn_hdim = 516\n",
    "batch_size = 16\n",
    "actual_epoch = 1\n",
    "num_fake_epoch = 20\n",
    "\n",
    "# Gradient descent parameters (I picked these by hand)\n",
    "# epsilon = 1 # learning rate for gradient descent\n",
    "# reg_lambda = 1 # regularization strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_input_shape=(23, 30)\n",
    "othr_input = (140, )\n",
    "\n",
    "conv_input = Input(shape=conv_input_shape, name='conv_input')\n",
    "x_conv = Conv1D(43, (3), padding='same', activation='relu')(conv_input)\n",
    "x_conv = Conv1D(43, (3), padding='same', activation='relu')(x_conv)\n",
    "x_conv = MaxPooling1D(pool_size=2)(x_conv)\n",
    "x_conv = Flatten()(x_conv)\n",
    "# x_conv = Dense(516, activation='relu', kernel_constraint=maxnorm(3))(x_conv)\n",
    "\n",
    "othr_input = Input(shape=othr_input, name='othr_input')\n",
    "# x_othr = Dense(256, input_dim=167, activation='relu')(othr_input)\n",
    "# x_othr = Dropout(0.3)(x_othr)\n",
    "# x_othr = Dense(512, activation='relu')(othr_input)\n",
    "# x_othr = Dropout(0.3)(x_othr)\n",
    "# x_othr = Dense(256, activation='relu')(othr_input)\n",
    "\n",
    "x = concatenate([x_conv, othr_input])\n",
    "# x = Dense(1024, input_dim=(516 + 256), activation='relu')(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(nn_output_dim, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=[conv_input, othr_input], outputs=x)\n",
    "\n",
    "# epochs = 5\n",
    "# lrate = 0.1\n",
    "# decay = lrate/epochs\n",
    "# sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "# model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Compile model\n",
    "# keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "opt = optimizers.Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[actual_accuracy,])\n",
    "# model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1345/1344 [==============================] - 20s - loss: 0.2174 - actual_accuracy: 0.2091 - val_loss: 0.1636 - val_actual_accuracy: 0.4882\n",
      "Epoch 2/20\n",
      "1345/1344 [==============================] - 22s - loss: 0.1543 - actual_accuracy: 0.4488 - val_loss: 0.1466 - val_actual_accuracy: 0.5261\n",
      "Epoch 3/20\n",
      "1345/1344 [==============================] - 24s - loss: 0.1375 - actual_accuracy: 0.5209 - val_loss: 0.1327 - val_actual_accuracy: 0.5959\n",
      "Epoch 4/20\n",
      " 323/1344 [======>.......................] - ETA: 13s - loss: 0.1291 - actual_accuracy: 0.5691"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(generate_fake_epoch(X_train, X_other_features_train, y_train, batch_size), X_train.shape[0]/(batch_size * num_fake_epoch), epochs=actual_epoch*num_fake_epoch, validation_data=([X_test, X_other_features_test], y_test))\n",
    "\n",
    "# model.fit([X_train, X_other_features_train], y_train, validation_data=([X_validate, X_other_features_validate], y_validate), epochs=1, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = '1_epoch'\n",
    "model.save(name + '.h5')\n",
    "output = open(name + '_history.pkl', 'wb')\n",
    "pickle.dump(history.history, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0f68f7e2798a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actual_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_actual_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['actual_accuracy'])\n",
    "plt.plot(history.history['val_actual_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model('test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h5f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e361f7023d53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'h5f' is not defined"
     ]
    }
   ],
   "source": [
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108736/108961 [============================>.] - ETA: 0s\n",
      "acc: 96.57%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate([X_validate, X_other_features_validate], y_validate)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nerazloènejši\n",
      "[ 0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "1.0\n",
      "1.0\n",
      "Elemwise{Cast{float32}}.0\n"
     ]
    }
   ],
   "source": [
    "pos = 0\n",
    "print(decode_input(X_validate[pos], dictionary))\n",
    "# predictions = model.predict([X_validate, X_other_features_validate])\n",
    "predictions2 = model.predict([X_validate[pos:pos+1], X_other_features_validate[pos:pos+1]])\n",
    "# print(predictions2)\n",
    "print(np.round(predictions2)[0])\n",
    "# predictions = np.round(predictions)\n",
    "print(y_validate[pos])\n",
    "# accuracy = sum([1 if np.all(y_validate[i] == predictions[i]) else 0 for i in range(X_validate.shape[0])])/float(X_validate.shape[0])\n",
    "# print(accuracy)\n",
    "\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "def act_accuracy(y_true, y_pred):\n",
    "    return K.mean(K.equal(K.mean(K.equal(K.round(y_true), K.round(y_pred)), axis=-1), 1.0))\n",
    "\n",
    "\n",
    "def mean_pred2(y_true, y_pred):\n",
    "    return K.mean(K.equal(K.argmax(y_true, axis=-1), K.argmax(y_pred, axis=-1)))\n",
    "#     return K.mean(K.all(K.equal(y_true, y_pred), axis=-1))\n",
    "#     return K.equal(K.round(y_true), K.round(y_pred))\n",
    "\n",
    "print(mean_pred(y_validate[pos], predictions[pos]).eval())\n",
    "print(mean_pred(np.array([[ 0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "                          [ 0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]), \n",
    "                np.array([[ 0.,  0.51,  0.,  0.51,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "                          [ 0.,  0.92,  0.,  0.51,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])).eval())\n",
    "# print(mean_pred(y_validate[pos], predictions[pos]))\n",
    "# print(mean_pred2(y_validate[pos], predictions[pos]))\n",
    "print(metrics.categorical_accuracy(y_validate[pos], predictions[pos]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X[429145:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_position(predictions[0], max_num_vowels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING CONTENT...\n",
      "CONTENT READ SUCCESSFULY\n",
      "CREATING DICTIONARY...\n",
      "DICTIONARY CREATION SUCCESSFUL!\n",
      "107287\n",
      "96.5186835311\n"
     ]
    }
   ],
   "source": [
    "def test_accuracy(predictions, y):\n",
    "    dictionary, max_word, max_num_vowels, content, vowels, accetuated_vowels = create_dict()\n",
    "    num_of_pred = len(predictions)\n",
    "    num_of_correct_pred = 0\n",
    "    for i in range(predictions.shape[0]):\n",
    "        if decode_position(predictions[i], max_num_vowels) == decode_position(y[i], max_num_vowels):\n",
    "            num_of_correct_pred += 1\n",
    "\n",
    "    return (num_of_correct_pred/float(num_of_pred)) * 100\n",
    "\n",
    "\n",
    "print(test_accuracy(predictions, y[429145:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107287, 121)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "print max_num_vowels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING CONTENT...\n",
      "CONTENT READ SUCCESSFULY\n",
      "CREATING DICTIONARY...\n",
      "DICTIONARY CREATION SUCCESSFUL!\n"
     ]
    }
   ],
   "source": [
    "dictionary, max_word, max_num_vowels, content, vowels, accetuated_vowels = create_dict()\n",
    "feature_dictionary = create_feature_dictionary()\n",
    "def generate_input_from_word(word, max_word, dictionary):\n",
    "    x = np.zeros((max_word, len(dictionary)))\n",
    "    j = 0\n",
    "    for c in list(word):\n",
    "        index = 0\n",
    "        for d in dictionary:\n",
    "            if c == d:\n",
    "                x[j, index] = 1\n",
    "                break\n",
    "            index += 1\n",
    "        j += 1\n",
    "    return x\n",
    "\n",
    "# model = load_model()\n",
    "# prediction = predict(model, generate_input_from_word('hidrija'))\n",
    "# print decode_position(prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../../prepare_data.py\n",
    "# generate_X_and_y(dictionary, max_word, max_num_vowels, content, vowels, accetuated_vowels, feature_dictionary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
